import torch
import torch.nn as nn
import torch.optim as optim
import torch.distributions as D
import matplotlib.pyplot as plt

# Coupling layer for a 1D flow, no splitting needed
class CouplingLayer(nn.Module):
    def __init__(self):
        super().__init__()
        self.scale_net = nn.Sequential(
            nn.Linear(1, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Tanh()
        )
        self.translate_net = nn.Sequential(
            nn.Linear(1, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )

    def forward(self, x, reverse=False):
        scale = self.scale_net(x)
        translation = self.translate_net(x)
        if not reverse:
            x = x * torch.exp(scale) + translation
        else:
            x = (x - translation) * torch.exp(-scale)
        return x, scale
        return x, scale

    def log_det_jacobian(self, x):
        scale = self.scale_net(x)
        return scale

# Normalizing flow model for 1D distribution
class NormalizingFlow(nn.Module):
    def __init__(self, num_flows):
        super().__init__()
        self.flows = nn.ModuleList([CouplingLayer() for _ in range(num_flows)])
        self.mean = nn.Parameter(torch.tensor(0.0))  # Trainable base mean
        self.var = nn.Parameter(torch.tensor(1.0))   # Trainable base variance
        self.base_dist = D.Normal(0, 1)

    def forward(self, x):
        log_det_jacobians = 0
        for flow in self.flows:
            x, log_det_jacobian = flow(x)
            log_det_jacobians += log_det_jacobian
        return x, log_det_jacobians

    def inverse(self, z):
        for flow in reversed(self.flows):
            z, _ = flow(z, reverse=True)
        return z

    def log_prob(self, x):
        z, log_det_jacobians = self.forward(x)
        log_base_prob = -0.5 * torch.log(2 * torch.pi * self.var**2) - (z - self.mean)**2 / (2 * self.var**2)
        return log_base_prob + log_det_jacobians

    def sample(self, num_samples):
        z = self.base_dist.sample((num_samples, 1))  # Ensures shape compatibility for 1D
        x = self.inverse(z)
        return x

# Training function
def train_flow():
    num_flows = 4
    target_dist = D.Normal(1.3, 0.4)  # 1D target distribution
    flow = NormalizingFlow(num_flows)
    optimizer = optim.Adam(flow.parameters(), lr=1e-3)
    num_steps = 5000

    for step in range(num_steps):
        x = target_dist.sample((128, 1))  # Ensure shape is [batch_size, 1] for 1D
        loss = -flow.log_prob(x).mean()   # Negative log likelihood

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if step % 1000 == 0:
            print(f"Step {step}: Loss = {loss.item()}")

    with torch.no_grad():
        generated_samples = flow.sample(5000).numpy()

    # Plot the generated samples
    plt.figure(figsize=(8, 6))
    plt.hist(generated_samples, bins=50, density=True, alpha=0.6, color='b', label="Generated Samples")
    x = torch.linspace(-2, 4, 1000).unsqueeze(1)
    plt.plot(x.numpy(), target_dist.log_prob(x).exp().numpy(), 'r', linewidth=2, label="Target PDF")
    plt.xlabel("Value")
    plt.ylabel("Density")
    plt.title("Samples Generated by Normalizing Flow")
    plt.legend()
    plt.show()

train_flow()
